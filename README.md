![alt text](public/banner.png)

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#-visÃ£o-geral)
- [Funcionalidades](#-funcionalidades)
- [InstalaÃ§Ã£o](#-instalaÃ§Ã£o)
- [UtilizaÃ§Ã£o](#-utilizaÃ§Ã£o)
- [Arquitectura](#-arquitectura)
- [Componentes TÃ©cnicos](#-componentes-tÃ©cnicos-detalhados)
- [Pipeline Completo](#-pipeline-completo)
- [ConfiguraÃ§Ã£o](#-configuraÃ§Ã£o-avanÃ§ada)
- [Performance](#-performance-e-optimizaÃ§Ãµes)
- [Exemplos](#-exemplos-de-uso)
- [Troubleshooting](#-troubleshooting)
- [Roadmap](#-roadmap-e-melhorias-futuras)

## ğŸ¯ VisÃ£o Geral

Este sistema implementa uma soluÃ§Ã£o completa de Information Retrieval para o RepositoriUM atravÃ©s de:

- **Embeddings SemÃ¢nticos**: Utiliza sentence transformers fine-tuned para capturar significado profundo dos documentos
- **Clustering Inteligente**: Reduz complexidade computacional atravÃ©s de agrupamento adaptativo de documentos
- **Cache HÃ­brido**: Sistema de cache em memÃ³ria e disco para mÃ¡xima performance
- **Query Processing**: NormalizaÃ§Ã£o e enhancement automÃ¡tico de queries de pesquisa
- **Similarity Multi-dimensional**: Combina TF-IDF, metadados e embeddings neurais de forma a obter melhores resultados

## âœ¨ Funcionalidades

### ğŸ”„ **Pipeline Automatizado Completo**
- ExtraÃ§Ã£o automÃ¡tica do RepositoriUM via protocolo OAI-PMH
- Processamento e limpeza de dados XML para formato JSON estruturado
- ValidaÃ§Ã£o rigorosa e remoÃ§Ã£o de duplicados
- CÃ¡lculo otimizado de similaridades com clustering adaptativo
- Fine-tuning de sentence transformers com dados especÃ­ficos do domÃ­nio
- Sistema de cache inteligente para embeddings
- Interface de pesquisa interativa em tempo real

### ğŸ§  **InteligÃªncia Artificial**
- **Sentence Transformers**: Embeddings contextuais de 384 dimensÃµes otimizados para o domÃ­nio acadÃ©mico
- **Clustering Adaptativo**: MiniBatch K-Means para eficiÃªncia computacional em grandes coleÃ§Ãµes
- **Query Enhancement**: Processamento inteligente que melhora a qualidade das pesquisas
- **Multi-modal Similarity**: CombinaÃ§Ã£o ponderada de TF-IDF, metadados e embeddings neurais

### âš¡ **OptimizaÃ§Ãµes de Performance**
- **Cache HÃ­brido**: CombinaÃ§Ã£o de memÃ³ria RAM e armazenamento persistente
- **VectorizaÃ§Ã£o NumPy**: OperaÃ§Ãµes SIMD para cÃ¡lculos matriciais eficientes
- **Batch Processing**: Processamento em lotes para maximizar throughput
- **Early Stopping**: Treino inteligente com validaÃ§Ã£o automÃ¡tica para evitar overfitting
- **Memory Management**: GestÃ£o otimizada de memÃ³ria para grandes coleÃ§Ãµes

### ğŸ“Š **Sistema de AvaliaÃ§Ã£o Robusto**
- **MÃ©tricas PadrÃ£o**: ImplementaÃ§Ã£o de Precision@K, Recall@K, MAP e MRR
- **ValidaÃ§Ã£o AutomÃ¡tica**: GeraÃ§Ã£o automÃ¡tica de test queries para avaliaÃ§Ã£o
- **Performance Monitoring**: EstatÃ­sticas detalhadas em tempo real
- **Quality Assurance**: ValidaÃ§Ã£o multi-fase de dados para garantir qualidade

## ğŸš€ InstalaÃ§Ã£o

### InstalaÃ§Ã£o das DependÃªncias
```bash
# Clona o repositÃ³rio
git clone https://github.com/username/ir-repositorium.git
cd ir-repositorium

# Instala dependÃªncias
pip install -r requirements.txt

# Download de recursos NLTK necessÃ¡rios
python -c "
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
"
```

### Estrutura do Projeto
```
src/
â”œâ”€â”€ main.py                    # Pipeline principal e orquestraÃ§Ã£o
â”œâ”€â”€ config.py                  # ConfiguraÃ§Ãµes globais do sistema
â”œâ”€â”€ utils.py                   # UtilitÃ¡rios partilhados entre componentes
â”œâ”€â”€ data_extraction.py         # ExtraÃ§Ã£o de dados via OAI-PMH
â”œâ”€â”€ data_processing.py         # Processamento XMLâ†’JSON
â”œâ”€â”€ data_validator.py          # ValidaÃ§Ã£o e limpeza de dados
â”œâ”€â”€ similarity_calculator.py   # CÃ¡lculo de similaridades com clustering
â”œâ”€â”€ model_trainer.py           # Fine-tuning de sentence transformers
â”œâ”€â”€ query_processor.py         # Processamento e enhancement de queries
â”œâ”€â”€ retrieval_system.py        # Motor de pesquisa semÃ¢ntica
â”œâ”€â”€ caching_system.py          # Sistema de cache hÃ­brido
â”œâ”€â”€ evaluation_system.py       # AvaliaÃ§Ã£o e mÃ©tricas de performance
â”œâ”€â”€ cache/                     # Armazenamento de embeddings em cache
â”œâ”€â”€ data/                      # Dados processados e estruturados
â””â”€â”€ models/                    # Modelos treinados e checkpoints
```

## ğŸ® UtilizaÃ§Ã£o

### ExecuÃ§Ã£o Completa do Pipeline

O sistema executa automaticamente todas as fases necessÃ¡rias atravÃ©s de um Ãºnico comando. 

```bash
python3 main.py
```

O pipeline inclui extraÃ§Ã£o de dados, processamento, validaÃ§Ã£o, cÃ¡lculo de similaridades, treino do modelo, prÃ©-computaÃ§Ã£o de embeddings e entrada em modo interativo de pesquisa.

### Modo Interativo de Pesquisa
ApÃ³s a conclusÃ£o do pipeline, o sistema pergunta ao utilizador se deseja entrar no modo interativo de pesquisa. Caso o utilizador opte por nÃ£o entrar no modo interativo, o sistema finaliza a execuÃ§Ã£o. Caso contrÃ¡rio, o sistema entra no mesmo, onde o utilizador pode realizar pesquisas em tempo real. Cada query Ã© processada, recebe enhancement automÃ¡tico, utiliza cache quando disponÃ­vel e apresenta resultados ordenados por relevÃ¢ncia (os mais similares).

### ExecuÃ§Ã£o de Componentes Individuais

Cada componente do sistema pode ser executado independentemente para desenvolvimento, debugging ou anÃ¡lise especÃ­fica de uma fase do pipeline.

```bash
python3 <nome_ficheiro>.py
```

## ğŸ—ï¸ Arquitectura

### Fluxo de Dados Principal

O sistema segue uma arquitectura pipeline onde os dados fluem sequencialmente atravÃ©s de mÃºltiplas fases de processamento. Inicia com a extraÃ§Ã£o de dados XML do RepositoriUM, processa para formato JSON estruturado, aplica validaÃ§Ã£o e limpeza, calcula similaridades com clustering, treina o modelo de embeddings, prÃ©-computa embeddings com cache e finaliza com o sistema de retrieval interativo.

### Arquitectura de Componentes

A arquitectura Ã© organizada em quatro camadas principais: Data Layer (dados brutos e processados), Processing Layer (extraÃ§Ã£o e processamento), ML Layer (treino e embeddings) e Retrieval Layer (pesquisa e avaliaÃ§Ã£o). Esta separaÃ§Ã£o permite modularidade, testabilidade e manutenibilidade do sistema.

## ğŸ§  Componentes TÃ©cnicos

### ğŸ“¥ **ExtraÃ§Ã£o de Dados (data_extraction.py)**

O sistema extrai documentos do RepositoriUM utilizando o protocolo OAI-PMH com mÃºltiplas otimizaÃ§Ãµes para robustez e eficiÃªncia.

#### **EstratÃ©gias de Robustez:**
- **Rate Limiting Inteligente**: Implementa delay adaptativo entre requests (1-3 segundos) para evitar sobrecarga do servidor e respeitar polÃ­ticas de uso
- **Retry com Backoff Exponencial**: Sistema de retry com atÃ© 3 tentativas e delays crescentes exponencialmente com jitter aleatÃ³rio para evitar thundering herd
- **Timeout ConfigurÃ¡vel**: Timeout de 45 segundos por request por defeito, ajustÃ¡vel conforme latÃªncia da rede e tamanho dos dados
- **GestÃ£o de Erros Consecutivos**: Para automaticamente apÃ³s 5 erros consecutivos para evitar loops infinitos e proteger o servidor remoto

#### **Suporte Multi-ColeÃ§Ã£o:**
O sistema suporta extraÃ§Ã£o simultÃ¢nea de mÃºltiplas coleÃ§Ãµes do RepositoriUM, incluindo Mestrados em InformÃ¡tica, outros Mestrados e Doutoramentos. A distribuiÃ§Ã£o Ã© feita de forma inteligente, monitorizando progresso e ajustando dinamicamente o nÃºmero de registos por coleÃ§Ã£o.

#### **OtimizaÃ§Ãµes de Performance:**
- **ExtraÃ§Ã£o por Lotes**: Utiliza resumption tokens do protocolo OAI-PMH para processar grandes volumes de dados eficientemente
- **Processamento Paralelo**: Metadados sÃ£o processados em paralelo durante a extraÃ§Ã£o para maximizar throughput
- **CompressÃ£o AutomÃ¡tica**: XML de saÃ­da Ã© comprimido automaticamente para poupar espaÃ§o de armazenamento
- **MonitorizaÃ§Ã£o em Tempo Real**: EstatÃ­sticas detalhadas sÃ£o apresentadas durante a extraÃ§Ã£o para acompanhamento do progresso

### ğŸ”§ **Processamento de Dados (data_processing.py)**

Converte dados XML para formato JSON estruturado atravÃ©s de um pipeline de limpeza multi-fase que garante qualidade e consistÃªncia dos dados.

#### **Pipeline de Limpeza:**

O sistema implementa um pipeline de limpeza em quatro fases principais:

1. **RemoÃ§Ã£o de Caracteres de Controlo**: Elimina caracteres nÃ£o imprimÃ­veis que podem corromper o processamento posterior, utilizando categorizaÃ§Ã£o Unicode para identificar caracteres problemÃ¡ticos.

2. **NormalizaÃ§Ã£o de EspaÃ§os**: Converte mÃºltiplos espaÃ§os, tabs e quebras de linha em espaÃ§os Ãºnicos atravÃ©s de expressÃµes regulares.

3. **Limpeza de Metadados**: Remove tags XML residuais, entidades HTML mal formadas e outros artefactos do processamento XML.

4. **NormalizaÃ§Ã£o de Datas**: Extrai anos no formato YYYY atravÃ©s de expressÃµes regulares para garantir consistÃªncia temporal nos metadados.

#### **ValidaÃ§Ã£o de Qualidade Rigorosa:**
- **Abstracts**: ValidaÃ§Ã£o de tamanho mÃ­nimo (50 caracteres) e mÃ¡ximo (2000 caracteres) para evitar ruÃ­do
- **TÃ­tulos**: VerificaÃ§Ã£o de obrigatoriedade e nÃ£o-vazio para garantir metadados essenciais
- **Metadados**: ValidaÃ§Ã£o de estrutura e tipos de dados para consistÃªncia
- **Encoding**: VerificaÃ§Ã£o e correÃ§Ã£o automÃ¡tica de problemas de codificaÃ§Ã£o UTF-8

#### **EstruturaÃ§Ã£o JSON:**
O sistema produz documentos JSON estruturados com campos normalizados incluindo identificador Ãºnico, tÃ­tulo limpo, abstract processado, lista de autores, keywords extraÃ­das, data normalizada, classificaÃ§Ãµes UDC e FoS, e memberships de coleÃ§Ãµes.

### ğŸ§® **CÃ¡lculo de Similaridades (similarity_calculator.py)**

Implementa uma abordagem hÃ­brida que combina clustering inteligente, TF-IDF otimizado e mÃºltiplas dimensÃµes de similaridade para criar dados de treino de alta qualidade.

#### **1. EstratÃ©gia de Clustering Adaptativo**

Para coleÃ§Ãµes grandes (> 1000 documentos), o sistema utiliza clustering para reduzir drasticamente a complexidade computacional de O(nÂ²) para O(n log n).

**ImplementaÃ§Ã£o MiniBatch K-Means:**
O sistema utiliza MiniBatch K-Means com nÃºmero de clusters adaptativo baseado no tamanho da coleÃ§Ã£o, processamento em lotes de 1000 documentos e seed fixo para reprodutibilidade.

**Vantagens do Clustering:**
- **EficiÃªncia Computacional**: ReduÃ§Ã£o dramÃ¡tica da complexidade algorÃ­tmica
- **Qualidade dos Pairs**: Documentos no mesmo cluster tÃªm alta probabilidade de similaridade semÃ¢ntica
- **Diversidade**: Pairs entre clusters diferentes fornecem exemplos negativos valiosos para treino
- **Escalabilidade**: Funciona eficientemente com dezenas de milhares de documentos

**EstratÃ©gia de Sampling Inteligente:**
O sistema gera pairs intra-cluster para alta similaridade (mÃ¡ximo 100 pairs por cluster) e pairs inter-cluster para baixa similaridade (33% de exemplos negativos), aplicando threshold de qualidade de 0.3 para filtrar pairs de baixa qualidade.

#### **2. TF-IDF HÃ­per-Otimizado**

O vectorizador TF-IDF Ã© configurado com parÃ¢metros cientificamente calibrados para maximizar informaÃ§Ã£o semÃ¢ntica enquanto mantÃ©m eficiÃªncia computacional.

**ConfiguraÃ§Ã£o Otimizada:**
- **max_features=5000**: VocabulÃ¡rio limitado aos 5000 termos mais informativos para equilibrar informaÃ§Ã£o vs. eficiÃªncia
- **ngram_range=(1,2)**: Unigramas e bigramas para capturar contexto e expressÃµes como "machine learning"
- **min_df=2**: Termos devem aparecer em pelo menos 2 documentos para eliminar typos e termos Ãºnicos
- **max_df=0.8**: Remove termos em mais de 80% dos documentos (palavras muito comuns que nÃ£o discriminam)

#### **3. Similaridade Multi-Dimensional AvanÃ§ada**

O sistema combina mÃºltiplos sinais de similaridade com pesos cientificamente calibrados para capturar diferentes aspectos da relevÃ¢ncia semÃ¢ntica.

**Componentes da Similaridade:**
- **Similaridade de Assuntos UDC/FoS**: 30% do peso total, utiliza Ã­ndice de Jaccard para conjuntos de classificaÃ§Ãµes
- **Similaridade de Keywords**: 20% do peso total, tambÃ©m com Ã­ndice de Jaccard para robustez
- **TF-IDF Base**: 70% do peso total, utilizando similaridade coseno para vectores densos

**JustificaÃ§Ã£o da PonderaÃ§Ã£o:**
A ponderaÃ§Ã£o 70/30 permite que a similaridade semÃ¢ntica TF-IDF domine (captura semÃ¢ntica profunda) enquanto os metadados refinam e ajustam (estrutura e classificaÃ§Ã£o formal).

### ğŸ§© **Processamento de Queries (query_processor.py)**

Sistema que normaliza e otimiza queries para maximizar a qualidade dos resultados de pesquisa atravÃ©s de um pipeline de processamento completo.

#### **Pipeline de Processamento:**

1. **Limpeza de Texto AvanÃ§ada**: Remove caracteres de controlo e normaliza espaÃ§os mÃºltiplos utilizando as mesmas funÃ§Ãµes do processamento de documentos para garantir consistÃªncia.

2. **TokenizaÃ§Ã£o Inteligente**: Utiliza o tokenizador NLTK punkt que lida corretamente com pontuaÃ§Ã£o, contraÃ§Ãµes e casos especiais da lÃ­ngua portuguesa e inglesa.

3. **RemoÃ§Ã£o de Stop Words Multi-idioma**: Sistema adaptativo que tenta portuguÃªs primeiro e faz fallback para inglÃªs, garantindo robustez em ambientes multilÃ­ngues.

4. **ExtraÃ§Ã£o de Keywords Filtrada**: Filtra tokens alfabÃ©ticos com mais de 2 caracteres, excluindo stop words para manter apenas termos semanticamente relevantes.

#### **ClassificaÃ§Ã£o AutomÃ¡tica de Queries:**
O sistema classifica automaticamente queries em categorias (empty, single_term, short_phrase, long_phrase) para permitir estratÃ©gias de pesquisa adaptadas ao tipo de query.

#### **EstratÃ©gias de Enhancement:**
Implementa duplicaÃ§Ã£o estratÃ©gica de keywords para reforÃ§o semÃ¢ntico, aumentando o peso TF-IDF dos termos importantes sem alterar a semÃ¢ntica fundamental da query.

#### **ConsistÃªncia com Documentos:**
O processamento de queries utiliza exactamente as mesmas funÃ§Ãµes (clean_text, extract_keywords) que o processamento de documentos, garantindo consistÃªncia perfeita na representaÃ§Ã£o textual entre queries e documentos.

### ğŸ¤– **Treino de Modelos (model_trainer.py)**

Implementa fine-tuning avanÃ§ado de sentence transformers com mÃºltiplas otimizaÃ§Ãµes para eficiÃªncia e qualidade, utilizando tÃ©cnicas de machine learning modernas.

#### **Modelo Base Estrategicamente Escolhido:**
Utiliza o modelo "sentence-transformers/all-MiniLM-L6-v2" que oferece o melhor compromisso entre tamanho (23M parÃ¢metros), velocidade (5x mais rÃ¡pido que modelos maiores), qualidade (mantÃ©m 95% da performance) e suporte multilÃ­ngue nativo.

#### **EstratÃ©gias de Treino:**

**1. Early Stopping:**
Implementa early stopping com paciÃªncia de 2 Ã©pocas, guardando checkpoints do melhor modelo e restaurando automaticamente quando a performance de validaÃ§Ã£o para de melhorar.

**2. Split AutomÃ¡tico de Dados:**
Quando nÃ£o sÃ£o fornecidos dados de validaÃ§Ã£o, o sistema automaticamente reserva 10% dos dados de treino para validaÃ§Ã£o, garantindo avaliaÃ§Ã£o robusta.

**3. ConfiguraÃ§Ã£o Adaptativa:**
DataLoader configurado com batch size otimizado para GPUs modernas (32), paralelizaÃ§Ã£o condicional baseada na disponibilidade de GPU e memory pinning para otimizaÃ§Ã£o de transferÃªncia de dados.

#### **Loss Function Especializada:**
Utiliza CosineSimilarityLoss que optimiza directamente a mÃ©trica usada no retrieval, oferece maior estabilidade que MSE e produz scores directamente interpretÃ¡veis como similaridade.

#### **AvaliaÃ§Ã£o RÃ¡pida Durante Treino:**
Sistema de avaliaÃ§Ã£o que utiliza apenas 200 exemplos para velocidade, processamento em batches para eficiÃªncia, embeddings vectorizados e correlaÃ§Ã£o de Pearson como mÃ©trica de qualidade.

### ğŸš€ **Sistema de Cache (caching_system.py)**

ImplementaÃ§Ã£o de cache hÃ­brido que combina memÃ³ria RAM e armazenamento persistente para mÃ¡xima performance e eficiÃªncia.

#### **Arquitectura do Cache HÃ­brido:**

**1. Memory Cache (Tier 1 - RAM):**
Cache em memÃ³ria com acesso O(1), zero I/O e substituiÃ§Ã£o LRU implÃ­cita quando atinge o limite configurÃ¡vel de 1000 embeddings.

**2. Disk Cache (Tier 2 - SSD/HDD):**
Cache persistente que sobrevive a reinicializaÃ§Ãµes, com capacidade ilimitada (limitada apenas pelo espaÃ§o em disco) e compressÃ£o automÃ¡tica via pickle.

#### **Sistema de Chaves Inteligente:**
Utiliza hash MD5 de uma combinaÃ§Ã£o modelo+texto para garantir que embeddings de modelos diferentes nÃ£o colidem, produzindo chaves de tamanho fixo independente do tamanho do texto e sendo determinÃ­stica para consistÃªncia.

#### **OperaÃ§Ãµes Batch:**
Implementa operaÃ§Ãµes batch para minimizar syscalls, garantir atomicidade de operaÃ§Ãµes em grupo e facilitar monitorizaÃ§Ã£o de progresso.

#### **EstratÃ©gia de Cache HierÃ¡rquico:**
Sistema de dois nÃ­veis onde o Tier 1 (memÃ³ria) Ã© verificado primeiro para mÃ¡xima velocidade, seguido do Tier 2 (disco) para persistÃªncia, com promoÃ§Ã£o automÃ¡tica de embeddings do disco para memÃ³ria quando hÃ¡ espaÃ§o disponÃ­vel.

### ğŸ” **Sistema de Retrieval (retrieval_system.py)**

Motor de pesquisa semÃ¢ntica que integra todos os componentes numa experiÃªncia de pesquisa fluida e eficiente.

#### **InicializaÃ§Ã£o com Cache Inteligente:**
Sistema inicializado com QueryProcessor para processamento de queries, EmbeddingCache para performance e carregamento automÃ¡tico do modelo treinado.

#### **PrÃ©-computaÃ§Ã£o de Embeddings com Cache:**
Verifica cache em batch para todos os abstracts, carrega instantaneamente se 100% cache hit, calcula apenas embeddings em falta se cache parcial, e reconstrÃ³i array completo mantendo ordem dos documentos.

#### **Retrieval com Processamento de Query Integrado:**
Pipeline completo que processa a query, aplica enhancement, verifica cache para embedding da query, calcula similaridades vectorizadas, aplica boost baseado em metadados e retorna resultados ordenados por relevÃ¢ncia.

#### **Similaridade SemÃ¢ntica Vectorizada:**
Implementa produto escalar normalizado (similaridade coseno) utilizando vectorizaÃ§Ã£o NumPy para operaÃ§Ãµes SIMD, broadcasting para evitar loops explÃ­citos e arrays contÃ­guos para eficiÃªncia de cache CPU.

#### **Sistema de Boost Inteligente:**
Aplica boost de 10% por match de keywords exactas, 15% por match no tÃ­tulo (mais importante), com cap mÃ¡ximo de 50% para evitar dominaÃ§Ã£o da similaridade semÃ¢ntica e preservaÃ§Ã£o da ordenaÃ§Ã£o relativa base.

### ğŸ“Š **Sistema de AvaliaÃ§Ã£o (evaluation_system.py)**

Framework completo de avaliaÃ§Ã£o que implementa mÃ©tricas padrÃ£o de Information Retrieval para validar rigorosamente a qualidade do sistema.

#### **MÃ©tricas de Retrieval Implementadas:**

**1. Precision@K:** Mede a proporÃ§Ã£o de documentos relevantes nos top-K resultados retornados.

**2. Recall@K:** Mede a proporÃ§Ã£o de documentos relevantes totais que foram recuperados nos top-K resultados.

**3. Mean Average Precision (MAP):** Calcula a mÃ©dia das precisÃµes em cada posiÃ§Ã£o onde um documento relevante Ã© encontrado.

**4. Mean Reciprocal Rank (MRR):** Calcula a mÃ©dia do inverso da posiÃ§Ã£o do primeiro documento relevante encontrado.

### ğŸ› ï¸ **ValidaÃ§Ã£o de Dados (data_validator.py)**

Sistema robusto de validaÃ§Ã£o que garante a qualidade e consistÃªncia dos dados atravÃ©s de mÃºltiplas fases de verificaÃ§Ã£o rigorosa.

#### **ValidaÃ§Ã£o XML PrÃ©via:**
Verifica integridade do XML antes do processamento, remove duplicados por identifier jÃ¡ na fase XML e produz estatÃ­sticas de duplicados encontrados.

#### **Pipeline de Limpeza Multi-Fase:**

**Fase 1 - RemoÃ§Ã£o de Duplicados:**
Remove duplicados por ID exacto primeiro, depois duplicados por conteÃºdo utilizando assinatura baseada em tÃ­tulo+autores+data normalizada.

**Fase 2 - ValidaÃ§Ã£o de Qualidade:**
Verifica obrigatoriedade de tÃ­tulos, valida tamanho de abstracts (mÃ­nimo 50, mÃ¡ximo 2000 caracteres), e remove documentos que nÃ£o cumprem critÃ©rios de qualidade.

## âš™ï¸ ConfiguraÃ§Ã£o AvanÃ§ada

### ParÃ¢metros de Performance

O sistema oferece configuraÃ§Ã£o detalhada de parÃ¢metros para clustering (sample ratio de 5%, clustering para coleÃ§Ãµes > 1000 docs), cache (1000 embeddings em memÃ³ria, cache persistente ativo), TF-IDF (vocabulÃ¡rio de 5000 features, min_df=2, max_df=0.8) e extraÃ§Ã£o (timeout de 45s, 3 retries, delay base de 1s).

### Modelo e Treino

ConfiguraÃ§Ã£o do modelo base all-MiniLM-L6-v2, 2 Ã©pocas de treino, batch size de 32, threshold de similaridade de 0.2 para pairs de treino, e validaÃ§Ã£o de abstracts entre 50-2000 caracteres.

## ğŸ› ï¸ Tecnologias Utilizadas

- **Sentence Transformers**: Framework para embeddings semÃ¢nticos
- **Scikit-learn**: Biblioteca para machine learning e TF-IDF
- **NLTK**: Toolkit para processamento de linguagem natural
- **NumPy**: Biblioteca para computaÃ§Ã£o cientÃ­fica e vectorizaÃ§Ã£o
- **PyTorch**: Framework para deep learning e neural networks

## ğŸ‘¥ Contribuidores

- [JoÃ£o Coelho - PG55954](https://github.com/JoaoCoelho2003)
- [JosÃ© Rodrigues - PG55969](https://github.com/FilipeR13)
- [Mariana Silva - PG55980](https://github.com/MarianaSilva659)

---